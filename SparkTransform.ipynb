{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, dayofmonth, dayofweek, month, quarter, year\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"TransactionProcessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Funom:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TransactionProcessing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f48f2d5810>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+--------+-----------+--------------+-------------------+\n",
      "|sale_id|customer_id|product_id|quantity|amount_paid|payment_method|          timestamp|\n",
      "+-------+-----------+----------+--------+-----------+--------------+-------------------+\n",
      "|   1392|       1451|         5|       1|      43.99|          Card|2023-01-01 17:12:48|\n",
      "|   1373|       1632|        14|       2|     199.98|          Card|2023-01-01 18:40:55|\n",
      "|   1145|       1848|        19|       1|      39.99|          Cash|2023-01-01 20:50:48|\n",
      "|   1225|       1906|        18|       3|     119.97|          Cash|2023-01-02 01:10:09|\n",
      "|   1430|        393|         8|       3|      44.97|          Card|2023-01-04 12:36:00|\n",
      "+-------+-----------+----------+--------+-----------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the transaction data into a Spark DataFrame\n",
    "transaction_df =spark.read.csv('data/raw/historical_transactions.csv', inferSchema=True, header=True)\n",
    "transaction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+--------+-----------+--------------+-------------------+----------+-------+\n",
      "|sale_id|customer_id|product_id|quantity|amount_paid|payment_method|          timestamp|sales_date|date_id|\n",
      "+-------+-----------+----------+--------+-----------+--------------+-------------------+----------+-------+\n",
      "|   1392|       1451|         5|       1|      43.99|          Card|2023-01-01 17:12:48|2023-01-01|   null|\n",
      "|   1373|       1632|        14|       2|     199.98|          Card|2023-01-01 18:40:55|2023-01-01|   null|\n",
      "|   1145|       1848|        19|       1|      39.99|          Cash|2023-01-01 20:50:48|2023-01-01|   null|\n",
      "|   1225|       1906|        18|       3|     119.97|          Cash|2023-01-02 01:10:09|2023-01-02|   null|\n",
      "|   1430|        393|         8|       3|      44.97|          Card|2023-01-04 12:36:00|2023-01-04|   null|\n",
      "+-------+-----------+----------+--------+-----------+--------------+-------------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'timestamp' column to datetime and extract the date component\n",
    "transaction_df = transaction_df.withColumn('sales_date', col('timestamp').cast('date'))\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Add a date_id column to the transaction DataFrame with constant values (None)\n",
    "transaction_df = transaction_df.withColumn('date_id', lit(None).cast('integer'))\n",
    "\n",
    "transaction_df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+--------+-----------+--------------+-------------------+----------+-------+\n",
      "|sale_id|customer_id|product_id|quantity|amount_paid|payment_method|          timestamp|sales_date|date_id|\n",
      "+-------+-----------+----------+--------+-----------+--------------+-------------------+----------+-------+\n",
      "|   1392|       1451|         5|       1|      43.99|          Card|2023-01-01 17:12:48|2023-01-01|      1|\n",
      "|   1373|       1632|        14|       2|     199.98|          Card|2023-01-01 18:40:55|2023-01-01|      1|\n",
      "|   1145|       1848|        19|       1|      39.99|          Cash|2023-01-01 20:50:48|2023-01-01|      1|\n",
      "|   1225|       1906|        18|       3|     119.97|          Cash|2023-01-02 01:10:09|2023-01-02|      2|\n",
      "|   1430|        393|         8|       3|      44.97|          Card|2023-01-04 12:36:00|2023-01-04|      3|\n",
      "+-------+-----------+----------+--------+-----------+--------------+-------------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Generate unique date IDs for each row in the DataFrame\n",
    "# We create a window specification to order rows by the 'sales_date' column, ensuring chronological order.\n",
    "window_spec = Window.orderBy(F.col(\"sales_date\"))\n",
    "\n",
    "# Using the window specification, we apply the dense_rank() function to assign a unique 'date_id' to each row.\n",
    "# The 'date_id' represents the sequential order of sales dates, allowing for time-based analysis.\n",
    "transaction_df = transaction_df.withColumn(\"date_id\", F.dense_rank().over(window_spec))\n",
    "\n",
    "\n",
    "# Show the DataFrame\n",
    "transaction_df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sale_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- sales_date: date (nullable = true)\n",
      " |-- date_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract additional date components\n",
    "transaction_df = transaction_df.withColumn('weekday', dayofweek(col('timestamp')))\n",
    "transaction_df = transaction_df.withColumn('month_name', date_format(col('timestamp'), 'MMMM'))\n",
    "transaction_df = transaction_df.withColumn('day', dayofmonth(col('timestamp')))\n",
    "transaction_df = transaction_df.withColumn('month', month(col('timestamp')))\n",
    "transaction_df = transaction_df.withColumn('quarter', quarter(col('timestamp')))\n",
    "transaction_df = transaction_df.withColumn('year', year(col('timestamp')))\n",
    "transaction_df = transaction_df.withColumn('weekday_name', date_format(col('timestamp'), 'EEEE'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+--------+-----------+--------------+-------------------+----------+-------+-------+----------+---+-----+-------+----+------------+------------+\n",
      "|sale_id|customer_id|product_id|quantity|amount_paid|payment_method|          timestamp|sales_date|date_id|weekday|month_name|day|month|quarter|year|weekday_name|quarter_name|\n",
      "+-------+-----------+----------+--------+-----------+--------------+-------------------+----------+-------+-------+----------+---+-----+-------+----+------------+------------+\n",
      "|   1392|       1451|         5|       1|      43.99|          Card|2023-01-01 17:12:48|2023-01-01|      1|      1|   January|  1|    1|      1|2023|      Sunday|          Q1|\n",
      "|   1373|       1632|        14|       2|     199.98|          Card|2023-01-01 18:40:55|2023-01-01|      1|      1|   January|  1|    1|      1|2023|      Sunday|          Q1|\n",
      "|   1145|       1848|        19|       1|      39.99|          Cash|2023-01-01 20:50:48|2023-01-01|      1|      1|   January|  1|    1|      1|2023|      Sunday|          Q1|\n",
      "|   1225|       1906|        18|       3|     119.97|          Cash|2023-01-02 01:10:09|2023-01-02|      2|      2|   January|  2|    1|      1|2023|      Monday|          Q1|\n",
      "|   1430|        393|         8|       3|      44.97|          Card|2023-01-04 12:36:00|2023-01-04|      3|      4|   January|  4|    1|      1|2023|   Wednesday|          Q1|\n",
      "+-------+-----------+----------+--------+-----------+--------------+-------------------+----------+-------+-------+----------+---+-----+-------+----+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Use the 'when' function to create the 'quarter_name' column\n",
    "transaction_df = transaction_df.withColumn('quarter_name',\n",
    "    when(col('quarter') == 1, 'Q1')\n",
    "    .when(col('quarter') == 2, 'Q2')\n",
    "    .when(col('quarter') == 3, 'Q3')\n",
    "    .when(col('quarter') == 4, 'Q4')\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "# Show the DataFrame\n",
    "transaction_df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for the dimension table\n",
    "date_columns = ['date_id', 'sales_date', 'year', 'quarter', 'quarter_name', 'month', 'month_name', 'day', 'weekday', 'weekday_name']\n",
    "raw_dim_date = transaction_df.select(date_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows based on the 'date_id' column\n",
    "dim_date = raw_dim_date.dropDuplicates(['date_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+-------+------------+-----+----------+---+-------+------------+\n",
      "|date_id|sales_date|year|quarter|quarter_name|month|month_name|day|weekday|weekday_name|\n",
      "+-------+----------+----+-------+------------+-----+----------+---+-------+------------+\n",
      "|      1|2023-01-01|2023|      1|          Q1|    1|   January|  1|      1|      Sunday|\n",
      "|      2|2023-01-02|2023|      1|          Q1|    1|   January|  2|      2|      Monday|\n",
      "|      3|2023-01-04|2023|      1|          Q1|    1|   January|  4|      4|   Wednesday|\n",
      "|      4|2023-01-05|2023|      1|          Q1|    1|   January|  5|      5|    Thursday|\n",
      "|      5|2023-01-07|2023|      1|          Q1|    1|   January|  7|      7|    Saturday|\n",
      "+-------+----------+----+-------+------------+-----+----------+---+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+--------+-----------+--------------+-------+\n",
      "|sale_id|customer_id|product_id|quantity|amount_paid|payment_method|date_id|\n",
      "+-------+-----------+----------+--------+-----------+--------------+-------+\n",
      "|   1392|       1451|         5|       1|      43.99|          Card|      1|\n",
      "|   1373|       1632|        14|       2|     199.98|          Card|      1|\n",
      "|   1145|       1848|        19|       1|      39.99|          Cash|      1|\n",
      "|   1225|       1906|        18|       3|     119.97|          Cash|      2|\n",
      "|   1430|        393|         8|       3|      44.97|          Card|      3|\n",
      "|   1421|       1154|         4|       1|     599.99|          Cash|      3|\n",
      "|   1339|         72|         4|       1|     599.99|          Cash|      4|\n",
      "|   1401|        310|        16|       4|     279.96|          Cash|      4|\n",
      "|   1108|       1697|         2|       1|     599.99|          Card|      4|\n",
      "|   1311|       1063|        24|       1|     129.99|          Card|      4|\n",
      "|   1302|        132|         2|       1|     599.99|          Card|      5|\n",
      "|   1199|       1155|         5|       1|      43.99|          Cash|      6|\n",
      "|   1015|        634|        13|       2|     199.98|          Card|      6|\n",
      "|   1223|       1477|        20|       1|      29.99|          Card|      7|\n",
      "|   1162|       1185|        11|       3|     449.97|          Card|      8|\n",
      "|   1148|        713|        13|       1|      99.99|          Cash|      9|\n",
      "|   1169|       1153|         9|       3|     269.97|          Card|      9|\n",
      "|   1118|       1533|         2|       1|     599.99|          Card|      9|\n",
      "|   1001|        478|        18|       4|     159.96|          Card|     10|\n",
      "|   1022|        299|         3|       1|     799.99|          Card|     11|\n",
      "+-------+-----------+----------+--------+-----------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select columns for the FactSales table\n",
    "fact_sales = transaction_df.select(['sale_id', 'customer_id', 'product_id', 'quantity', 'amount_paid', 'payment_method', 'date_id'])\n",
    "fact_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
